{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install pyserini==0.25.0 pytrec_eval datasets tqdm\n",
    "!pip install faiss-cpu"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Highly recommended to run the code on Google Colab: https://colab.research.google.com/drive/1Mdo5yRB1Sz4nJ5gtNbe1y9DJz8TPDfBv?usp=sharing\n",
    "The code partially from https://huggingface.co/datasets/intfloat/query2doc_msmarco/blob/main/repro_bm25.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import faiss\n",
    "import urllib.request\n",
    "import json\n",
    "import tqdm\n",
    "import pytrec_eval\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from typing import Dict, Tuple\n",
    "from datasets import load_dataset\n",
    "from pyserini.search import SimpleSearcher"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_means(my_results):\n",
    "  means = {metric: np.mean([d[metric] for d in my_results]) for metric in my_results[0]}\n",
    "\n",
    "  # Calculate standard deviations\n",
    "  std_devs = {metric: np.std([d[metric] for d in my_results]) for metric in my_results[0]}\n",
    "\n",
    "  # Print means\n",
    "  print(\"Means:\")\n",
    "  for metric, mean_value in means.items():\n",
    "      print(f\"{metric}: {mean_value}\")\n",
    "\n",
    "  # Print standard deviations\n",
    "  print(\"\\nStandard Deviations:\")\n",
    "  for metric, std_value in std_devs.items():\n",
    "      print(f\"{metric}: {std_value}\")\n",
    "\n",
    "def trec_eval(qrels: Dict[str, Dict[str, int]],\n",
    "              results: Dict[str, Dict[str, float]],\n",
    "              k_values: Tuple[int] = (10, 50, 100, 200, 1000)) -> Dict[str, float]:\n",
    "    ndcg, _map, recall = {}, {}, {}\n",
    "\n",
    "    for k in k_values:\n",
    "        ndcg[f\"NDCG@{k}\"] = 0.0\n",
    "        _map[f\"MAP@{k}\"] = 0.0\n",
    "        recall[f\"Recall@{k}\"] = 0.0\n",
    "\n",
    "    map_string = \"map_cut.\" + \",\".join([str(k) for k in k_values])\n",
    "    ndcg_string = \"ndcg_cut.\" + \",\".join([str(k) for k in k_values])\n",
    "    recall_string = \"recall.\" + \",\".join([str(k) for k in k_values])\n",
    "\n",
    "    evaluator = pytrec_eval.RelevanceEvaluator(qrels, {map_string, ndcg_string, recall_string})\n",
    "    scores = evaluator.evaluate(results)\n",
    "\n",
    "    for query_id in scores:\n",
    "        for k in k_values:\n",
    "            ndcg[f\"NDCG@{k}\"] += scores[query_id][\"ndcg_cut_\" + str(k)]\n",
    "            _map[f\"MAP@{k}\"] += scores[query_id][\"map_cut_\" + str(k)]\n",
    "            recall[f\"Recall@{k}\"] += scores[query_id][\"recall_\" + str(k)]\n",
    "\n",
    "    def _normalize(m: dict) -> dict:\n",
    "        return {k: round(v / len(scores), 5) for k, v in m.items()}\n",
    "\n",
    "    ndcg = _normalize(ndcg)\n",
    "    _map = _normalize(_map)\n",
    "    recall = _normalize(recall)\n",
    "\n",
    "    all_metrics = {}\n",
    "    for mt in [ndcg, _map, recall]:\n",
    "        all_metrics.update(mt)\n",
    "\n",
    "    return all_metrics\n",
    "\n",
    "\n",
    "def load_qrels_from_file(file_path: str) -> Dict[str, Dict[str, int]]:\n",
    "    qrels = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            qid, _, pid, score = line.strip().split()\n",
    "            if qid not in qrels:\n",
    "                qrels[qid] = {}\n",
    "            qrels[qid][pid] = int(score)\n",
    "    print('Load {} queries {} qrels from {}'.format(len(qrels), sum(len(v) for v in qrels.values()), file_path))\n",
    "    return qrels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "query2doc_dataset = load_dataset('intfloat/query2doc_msmarco')['train']\n",
    "with open('long_tail_queries.txt', 'r') as file:\n",
    "  ids = [line.strip() for line in file]\n",
    "random_ids_long_tail = [example['query_id'] for example in query2doc_dataset if example['query_id'] in ids]\n",
    "random_ids_common = [example['query_id'] for example in query2doc_dataset if example['query_id'] not in ids]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "my_results_common = []\n",
    "def main(split: str = 'train'):\n",
    "    searcher: SimpleSearcher = SimpleSearcher.from_prebuilt_index('msmarco-passage')\n",
    "\n",
    "    for i in range(5):\n",
    "      # random_ids_my_common = random.sample(random_ids_common, k=50)\n",
    "      random_ids_long_my_tail = random.sample(random_ids_long_tail, k=50)\n",
    "      query2doc_my_dataset = query2doc_dataset.filter(lambda example: example['query_id'] in random_ids_long_my_tail)\n",
    "      queries = []\n",
    "      for idx in range(len(query2doc_my_dataset)):\n",
    "          example = query2doc_my_dataset[idx]\n",
    "          new_query = '{} {}'.format(' '.join([example['query'] for _ in range(5)]), example['pseudo_doc'])\n",
    "          queries.append(new_query)\n",
    "      print('Load {} queries'.format(len(queries)))\n",
    "\n",
    "      results: Dict[str, Dict[str, float]] = {}\n",
    "      batch_size = 64\n",
    "      num_batches = (len(queries) + batch_size - 1) // batch_size\n",
    "      for i in tqdm.tqdm(range(num_batches), mininterval=2):\n",
    "          batch_query_ids = query2doc_my_dataset['query_id'][i * batch_size: (i + 1) * batch_size]\n",
    "          batch_queries = queries[i * batch_size: (i + 1) * batch_size]\n",
    "          qid_to_hits: dict = searcher.batch_search(batch_queries, qids=batch_query_ids, k=1000, threads=8)\n",
    "          for qid, hits in qid_to_hits.items():\n",
    "              results[qid] = {hit.docid: hit.score for hit in hits}\n",
    "\n",
    "      qrels_file_path = 'qrels_train.tsv'\n",
    "      qrels = load_qrels_from_file(qrels_file_path)\n",
    "\n",
    "      all_metrics = trec_eval(qrels=qrels, results=results)\n",
    "      my_results_common.append(all_metrics)\n",
    "\n",
    "    # print('Evaluation results for {} split:'.format(split))\n",
    "    # print(json.dumps(all_metrics, ensure_ascii=False, indent=4))\n",
    "    calculate_means(my_results_common)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(split='train')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyserini.search.faiss import FaissSearcher, TctColBertQueryEncoder\n",
    "\n",
    "\n",
    "def main(split: str = 'train'):\n",
    "    encoder = TctColBertQueryEncoder('castorini/tct_colbert-msmarco')\n",
    "    searcher = FaissSearcher.from_prebuilt_index(\n",
    "        'msmarco-passage-tct_colbert-hnsw',\n",
    "        encoder\n",
    "    )\n",
    "    # random_ids_my_common = random.sample(random_ids_common, k=50)\n",
    "    random_ids_long_my_tail = random.sample(random_ids_long_tail, k=50)\n",
    "    query2doc_my_dataset = query2doc_dataset.filter(lambda example: example['query_id'] in random_ids_long_my_tail)\n",
    "\n",
    "    queries = []\n",
    "    for idx in range(len(query2doc_my_dataset)):\n",
    "        example = query2doc_my_dataset[idx]\n",
    "        new_query = '{} {}'.format(' '.join([example['query'] for _ in range(5)]), example['pseudo_doc'])\n",
    "        queries.append(new_query)\n",
    "    print('Load {} queries'.format(len(queries)))\n",
    "\n",
    "    results: Dict[str, Dict[str, float]] = {}\n",
    "    batch_size = 64\n",
    "    num_batches = (len(queries) + batch_size - 1) // batch_size\n",
    "    for i in tqdm.tqdm(range(num_batches), mininterval=2):\n",
    "        batch_query_ids = query2doc_my_dataset['query_id'][i * batch_size: (i + 1) * batch_size]\n",
    "        batch_queries = queries[i * batch_size: (i + 1) * batch_size]\n",
    "        qid_to_hits: dict = searcher.batch_search(batch_queries, qids=batch_query_ids, k=1000, threads=8)\n",
    "        for qid, hits in qid_to_hits.items():\n",
    "            results[qid] = {hit.docid: hit.score for hit in hits}\n",
    "    qrels_file_path = 'qrels_train.tsv'\n",
    "    qrels = load_qrels_from_file(qrels_file_path, ids)\n",
    "    all_metrics = trec_eval(qrels=qrels, results=results)\n",
    "\n",
    "    print('Evaluation results for {} split:'.format(split))\n",
    "    print(json.dumps(all_metrics, ensure_ascii=False, indent=4))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(split='train')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "my_results_common = []\n",
    "def main(split: str = 'train'):\n",
    "    searcher: SimpleSearcher = SimpleSearcher.from_prebuilt_index('msmarco-passage')\n",
    "\n",
    "    for i in range(5):\n",
    "      random_ids_my_common = random.sample(random_ids_common, k=50)\n",
    "      # random_ids_long_my_tail = random.sample(random_ids_long_tail, k=50)\n",
    "      query2doc_my_dataset = query2doc_dataset.filter(lambda example: example['query_id'] in random_ids_my_common)\n",
    "      queries = []\n",
    "      for idx in range(len(query2doc_my_dataset)):\n",
    "          example = query2doc_my_dataset[idx]\n",
    "          new_query = '{}'.format(' '.join([example['query'] for _ in range(5)]))\n",
    "          queries.append(new_query)\n",
    "      print('Load {} queries'.format(len(queries)))\n",
    "\n",
    "      results: Dict[str, Dict[str, float]] = {}\n",
    "      batch_size = 64\n",
    "      num_batches = (len(queries) + batch_size - 1) // batch_size\n",
    "      for i in tqdm.tqdm(range(num_batches), mininterval=2):\n",
    "          batch_query_ids = query2doc_my_dataset['query_id'][i * batch_size: (i + 1) * batch_size]\n",
    "          batch_queries = queries[i * batch_size: (i + 1) * batch_size]\n",
    "          qid_to_hits: dict = searcher.batch_search(batch_queries, qids=batch_query_ids, k=1000, threads=8)\n",
    "          for qid, hits in qid_to_hits.items():\n",
    "              results[qid] = {hit.docid: hit.score for hit in hits}\n",
    "\n",
    "      qrels_file_path = 'qrels_train.tsv'\n",
    "      qrels = load_qrels_from_file(qrels_file_path)\n",
    "\n",
    "      all_metrics = trec_eval(qrels=qrels, results=results)\n",
    "      my_results_common.append(all_metrics)\n",
    "\n",
    "    # print('Evaluation results for {} split:'.format(split))\n",
    "    # print(json.dumps(all_metrics, ensure_ascii=False, indent=4))\n",
    "    calculate_means(my_results_common)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(split='train')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
